{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_frankenstein.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIZGbpoIZrxbJ9AdtAk8zH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgorkemuysal/TextGenerationFrankenstein/blob/master/text_generation_frankenstein.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyYMaAAUrp0r",
        "colab_type": "text"
      },
      "source": [
        "# **Downloading and Preparing the Data**\n",
        "I downloaded the data from Project Gutenberg website (https://www.gutenberg.org/browse/scores/top) and uploaded to my Github repository to get and use easily. After downloading, to make data useful for processing, we should read the data in lowercase and create an array of all text  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj7xbv-q_nAF",
        "colab_type": "code",
        "outputId": "152d23b9-622d-43bf-f61e-3959b9da7460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/mgorkemuysal/TextGenerationFrankenstein.git\n",
        "# Nothing is so painful to the human mind as a great and sudden change \n",
        "import string\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM \n",
        "from keras.optimizers import RMSprop\n",
        "import random as rd \n",
        "import sys\n",
        "np.seterr(divide = 'ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TextGenerationFrankenstein'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "Unpacking objects:  14% (1/7)   \rUnpacking objects:  28% (2/7)   \rUnpacking objects:  42% (3/7)   \rUnpacking objects:  57% (4/7)   \rUnpacking objects:  71% (5/7)   \rremote: Total 7 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  85% (6/7)   \rUnpacking objects: 100% (7/7)   \rUnpacking objects: 100% (7/7), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4gZ_f9ct-b_",
        "colab_type": "text"
      },
      "source": [
        " # **Inspecting and Cleaning the Data**\n",
        " We have a book of 440748 characters including punctuations and special characters. To improve our vocabulary and modeling process, we must get rid of these punctuations and special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZn-n4fQ_9oA",
        "colab_type": "code",
        "outputId": "46d3c956-b080-4f1e-e2bd-ddbf48e70e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "raw_text = open('./TextGenerationFrankenstein/frankenstein.txt', 'rt').read().lower()\n",
        "print('Corpus lenght:', len(raw_text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus lenght: 440748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGG6U1Y1Ap1m",
        "colab_type": "code",
        "outputId": "c05a472c-ab22-49a6-b22e-11e3d78d9604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(raw_text[:5000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\n",
            "project gutenberg's frankenstein, by mary wollstonecraft (godwin) shelley\n",
            "\n",
            "this ebook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  you may copy it, give it away or\n",
            "re-use it under the terms of the project gutenberg license included\n",
            "with this ebook or online at www.gutenberg.net\n",
            "\n",
            "\n",
            "title: frankenstein\n",
            "       or the modern prometheus\n",
            "\n",
            "author: mary wollstonecraft (godwin) shelley\n",
            "\n",
            "release date: june 17, 2008 [ebook #84]\n",
            "last updated: january 13, 2018\n",
            "\n",
            "language: english\n",
            "\n",
            "character set encoding: utf-8\n",
            "\n",
            "*** start of this project gutenberg ebook frankenstein ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "produced by judith boss, christy phillips, lynn hanninen,\n",
            "and david meltzer. html version by al haines.\n",
            "further corrections by menno de leeuw.\n",
            "\n",
            "\n",
            "\n",
            "frankenstein;\n",
            "\n",
            "\n",
            "or, the modern prometheus\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "by\n",
            "\n",
            "\n",
            "mary wollstonecraft (godwin) shelley\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "contents\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "letter 1\n",
            "\n",
            "letter 2\n",
            "\n",
            "letter 3\n",
            "\n",
            "letter 4\n",
            "\n",
            "chapter 1\n",
            "\n",
            "chapter 2\n",
            "\n",
            "chapter 3\n",
            "\n",
            "chapter 4\n",
            "\n",
            "chapter 5\n",
            "\n",
            "chapter 6\n",
            "\n",
            "chapter 7\n",
            "\n",
            "chapter 8\n",
            "\n",
            "chapter 9\n",
            "\n",
            "chapter 10\n",
            "\n",
            "chapter 11\n",
            "\n",
            "chapter 12\n",
            "\n",
            "chapter 13\n",
            "\n",
            "chapter 14\n",
            "\n",
            "chapter 15\n",
            "\n",
            "chapter 16\n",
            "\n",
            "chapter 17\n",
            "\n",
            "chapter 18\n",
            "\n",
            "chapter 19\n",
            "\n",
            "chapter 20\n",
            "\n",
            "chapter 21\n",
            "\n",
            "chapter 22\n",
            "\n",
            "chapter 23\n",
            "\n",
            "chapter 24\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "letter 1\n",
            "\n",
            "_to mrs. saville, england._\n",
            "\n",
            "\n",
            "st. petersburgh, dec. 11th, 17—.\n",
            "\n",
            "\n",
            "you will rejoice to hear that no disaster has accompanied the\n",
            "commencement of an enterprise which you have regarded with such evil\n",
            "forebodings.  i arrived here yesterday, and my first task is to assure\n",
            "my dear sister of my welfare and increasing confidence in the success\n",
            "of my undertaking.\n",
            "\n",
            "i am already far north of london, and as i walk in the streets of\n",
            "petersburgh, i feel a cold northern breeze play upon my cheeks, which\n",
            "braces my nerves and fills me with delight.  do you understand this\n",
            "feeling?  this breeze, which has travelled from the regions towards\n",
            "which i am advancing, gives me a foretaste of those icy climes.\n",
            "inspirited by this wind of promise, my daydreams become more fervent\n",
            "and vivid.  i try in vain to be persuaded that the pole is the seat of\n",
            "frost and desolation; it ever presents itself to my imagination as the\n",
            "region of beauty and delight.  there, margaret, the sun is for ever\n",
            "visible, its broad disk just skirting the horizon and diffusing a\n",
            "perpetual splendour.  there—for with your leave, my sister, i will put\n",
            "some trust in preceding navigators—there snow and frost are banished;\n",
            "and, sailing over a calm sea, we may be wafted to a land surpassing in\n",
            "wonders and in beauty every region hitherto discovered on the habitable\n",
            "globe.  its productions and features may be without example, as the\n",
            "phenomena of the heavenly bodies undoubtedly are in those undiscovered\n",
            "solitudes.  what may not be expected in a country of eternal light?  i\n",
            "may there discover the wondrous power which attracts the needle and may\n",
            "regulate a thousand celestial observations that require only this\n",
            "voyage to render their seeming eccentricities consistent for ever.  i\n",
            "shall satiate my ardent curiosity with the sight of a part of the world\n",
            "never before visited, and may tread a land never before imprinted by\n",
            "the foot of man. these are my enticements, and they are sufficient to\n",
            "conquer all fear of danger or death and to induce me to commence this\n",
            "laborious voyage with the joy a child feels when he embarks in a little\n",
            "boat, with his holiday mates, on an expedition of discovery up his\n",
            "native river. but supposing all these conjectures to be false, you\n",
            "cannot contest the inestimable benefit which i shall confer on all\n",
            "mankind, to the last generation, by discovering a passage near the pole\n",
            "to those countries, to reach which at present so many months are\n",
            "requisite; or by ascertaining the secret of the magnet, which, if at\n",
            "all possible, can only be effected by an undertaking such as mine.\n",
            "\n",
            "these reflections have dispelled the agitation with which i began my\n",
            "letter, and i feel my heart glow with an enthusiasm which elevates me\n",
            "to heaven, for nothing contributes so much to tranquillise the mind as\n",
            "a steady purpose—a point on which the soul may fix its intellectual\n",
            "eye.  this expedition has been the favourite dream of my early years. i\n",
            "have read with ardour the accounts of the various voyages which have\n",
            "been made in the prospect of arriving at the north pacific ocean\n",
            "through the seas which surround the pole.  you may remember that a\n",
            "history of all the voyages made for purposes of discovery composed the\n",
            "whole of our good uncle thomas’ library.  my education was neglected,\n",
            "yet i was passionately fond of reading.  these volumes were my study\n",
            "day and night, and my familiarity with them increased that regret which\n",
            "i had felt, as a child, on learning that my father’s dying injunction\n",
            "had forbidden my uncle to allow me to embark in a seafaring life.\n",
            "\n",
            "these visions faded when i perused, for the first time, those poets\n",
            "whose effusions entranced my soul and lifted it to heaven.  i also\n",
            "became a poet and for one year lived in a paradise of my own creation;\n",
            "i imagined that i also might obtain a niche in the temple \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b282VEX3uvQ5",
        "colab_type": "code",
        "outputId": "513a47a4-95ff-4b50-bbcb-fb983d7df117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "punc_list = list(string.punctuation) + ['“', '”', 'æ', 'è', 'é', 'ê', 'ô', '—', '‘', '’', '\\ufeff']\n",
        "print(punc_list)\n",
        " \n",
        "def remove_punctuations(txt):\n",
        "  for c in punc_list:\n",
        "    txt = txt.replace(c, '')\n",
        "  return txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '“', '”', 'æ', 'è', 'é', 'ê', 'ô', '—', '‘', '’', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwfyPzxny2BB",
        "colab_type": "code",
        "outputId": "17581f26-6338-4ac8-9f91-61946c6f3738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "text = remove_punctuations(raw_text)\n",
        "print(text[:5000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "project gutenbergs frankenstein by mary wollstonecraft godwin shelley\n",
            "\n",
            "this ebook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever  you may copy it give it away or\n",
            "reuse it under the terms of the project gutenberg license included\n",
            "with this ebook or online at wwwgutenbergnet\n",
            "\n",
            "\n",
            "title frankenstein\n",
            "       or the modern prometheus\n",
            "\n",
            "author mary wollstonecraft godwin shelley\n",
            "\n",
            "release date june 17 2008 ebook 84\n",
            "last updated january 13 2018\n",
            "\n",
            "language english\n",
            "\n",
            "character set encoding utf8\n",
            "\n",
            " start of this project gutenberg ebook frankenstein \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "produced by judith boss christy phillips lynn hanninen\n",
            "and david meltzer html version by al haines\n",
            "further corrections by menno de leeuw\n",
            "\n",
            "\n",
            "\n",
            "frankenstein\n",
            "\n",
            "\n",
            "or the modern prometheus\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "by\n",
            "\n",
            "\n",
            "mary wollstonecraft godwin shelley\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "contents\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "letter 1\n",
            "\n",
            "letter 2\n",
            "\n",
            "letter 3\n",
            "\n",
            "letter 4\n",
            "\n",
            "chapter 1\n",
            "\n",
            "chapter 2\n",
            "\n",
            "chapter 3\n",
            "\n",
            "chapter 4\n",
            "\n",
            "chapter 5\n",
            "\n",
            "chapter 6\n",
            "\n",
            "chapter 7\n",
            "\n",
            "chapter 8\n",
            "\n",
            "chapter 9\n",
            "\n",
            "chapter 10\n",
            "\n",
            "chapter 11\n",
            "\n",
            "chapter 12\n",
            "\n",
            "chapter 13\n",
            "\n",
            "chapter 14\n",
            "\n",
            "chapter 15\n",
            "\n",
            "chapter 16\n",
            "\n",
            "chapter 17\n",
            "\n",
            "chapter 18\n",
            "\n",
            "chapter 19\n",
            "\n",
            "chapter 20\n",
            "\n",
            "chapter 21\n",
            "\n",
            "chapter 22\n",
            "\n",
            "chapter 23\n",
            "\n",
            "chapter 24\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "letter 1\n",
            "\n",
            "to mrs saville england\n",
            "\n",
            "\n",
            "st petersburgh dec 11th 17\n",
            "\n",
            "\n",
            "you will rejoice to hear that no disaster has accompanied the\n",
            "commencement of an enterprise which you have regarded with such evil\n",
            "forebodings  i arrived here yesterday and my first task is to assure\n",
            "my dear sister of my welfare and increasing confidence in the success\n",
            "of my undertaking\n",
            "\n",
            "i am already far north of london and as i walk in the streets of\n",
            "petersburgh i feel a cold northern breeze play upon my cheeks which\n",
            "braces my nerves and fills me with delight  do you understand this\n",
            "feeling  this breeze which has travelled from the regions towards\n",
            "which i am advancing gives me a foretaste of those icy climes\n",
            "inspirited by this wind of promise my daydreams become more fervent\n",
            "and vivid  i try in vain to be persuaded that the pole is the seat of\n",
            "frost and desolation it ever presents itself to my imagination as the\n",
            "region of beauty and delight  there margaret the sun is for ever\n",
            "visible its broad disk just skirting the horizon and diffusing a\n",
            "perpetual splendour  therefor with your leave my sister i will put\n",
            "some trust in preceding navigatorsthere snow and frost are banished\n",
            "and sailing over a calm sea we may be wafted to a land surpassing in\n",
            "wonders and in beauty every region hitherto discovered on the habitable\n",
            "globe  its productions and features may be without example as the\n",
            "phenomena of the heavenly bodies undoubtedly are in those undiscovered\n",
            "solitudes  what may not be expected in a country of eternal light  i\n",
            "may there discover the wondrous power which attracts the needle and may\n",
            "regulate a thousand celestial observations that require only this\n",
            "voyage to render their seeming eccentricities consistent for ever  i\n",
            "shall satiate my ardent curiosity with the sight of a part of the world\n",
            "never before visited and may tread a land never before imprinted by\n",
            "the foot of man these are my enticements and they are sufficient to\n",
            "conquer all fear of danger or death and to induce me to commence this\n",
            "laborious voyage with the joy a child feels when he embarks in a little\n",
            "boat with his holiday mates on an expedition of discovery up his\n",
            "native river but supposing all these conjectures to be false you\n",
            "cannot contest the inestimable benefit which i shall confer on all\n",
            "mankind to the last generation by discovering a passage near the pole\n",
            "to those countries to reach which at present so many months are\n",
            "requisite or by ascertaining the secret of the magnet which if at\n",
            "all possible can only be effected by an undertaking such as mine\n",
            "\n",
            "these reflections have dispelled the agitation with which i began my\n",
            "letter and i feel my heart glow with an enthusiasm which elevates me\n",
            "to heaven for nothing contributes so much to tranquillise the mind as\n",
            "a steady purposea point on which the soul may fix its intellectual\n",
            "eye  this expedition has been the favourite dream of my early years i\n",
            "have read with ardour the accounts of the various voyages which have\n",
            "been made in the prospect of arriving at the north pacific ocean\n",
            "through the seas which surround the pole  you may remember that a\n",
            "history of all the voyages made for purposes of discovery composed the\n",
            "whole of our good uncle thomas library  my education was neglected\n",
            "yet i was passionately fond of reading  these volumes were my study\n",
            "day and night and my familiarity with them increased that regret which\n",
            "i had felt as a child on learning that my fathers dying injunction\n",
            "had forbidden my uncle to allow me to embark in a seafaring life\n",
            "\n",
            "these visions faded when i perused for the first time those poets\n",
            "whose effusions entranced my soul and lifted it to heaven  i also\n",
            "became a poet and for one year lived in a paradise of my own creation\n",
            "i imagined that i also might obtain a niche in the temple where the\n",
            "names of homer and shakespeare are consecrated  you are well\n",
            "acquainted with my failure and how heavily i \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH-UvEESva4H",
        "colab_type": "text"
      },
      "source": [
        "# **Vectorizing Sequences of Characters**\n",
        "Let's split the book text up into subsequences with a fixed length of 60 characters. Each training pattern of the network is comprised of 60 time steps of one character(x) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 60 characters that preceded it. \n",
        "\n",
        "Also let's define a fixed step of 3. this step means that model in prediction phase will predict next character according to preceding 3 characters. For example: ex --> exa, xa --> xam, am --> ampl, mpl --> mple.\n",
        "\n",
        "Another thing we have to do, convert the characters into integers because we cannot build and train a model with characters. Also since we want to build a character-level text generation, we need these character indices as tokens. To do this we must map these characters to indices like key-value pairs.\n",
        "\n",
        "As result, we 26 letters of alphabet, 10 figures, newline and spaces as mapped.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pm8XZNZ0JR4",
        "colab_type": "code",
        "outputId": "ea8d4ece-d861-4787-fb09-c3c2195601d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "maxlen = 60\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        " \n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "  sentences.append(text[i: i + maxlen])\n",
        "  next_chars.append(text[i + maxlen])\n",
        " \n",
        "print('Number of Sequences:', len(sentences))\n",
        " \n",
        "chars = sorted(list(set(text)))\n",
        "print('\\nCharacters:', chars)\n",
        "print('Unique Characters:', len(chars))\n",
        " \n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "print('\\nCharacter Dictionary:', char_indices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Sequences: 143120\n",
            "\n",
            "Characters: ['\\n', ' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Unique Characters: 38\n",
            "\n",
            "Character Dictionary: {'\\n': 0, ' ': 1, '0': 2, '1': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bsd_vL437bh",
        "colab_type": "text"
      },
      "source": [
        "# **Transforming Data for Model Building**\n",
        "Because of the lstm networks need tensors as input and target, we need to create 3d tensor (numpy array x of shape(sequences, maxlen, unique characters)) for lstm input shape and 2d tensor (numpy array y of shape(sequnces, unique characters)) for target. \n",
        "\n",
        "To do this, we must one-hot encode the sentences we created before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2HmHpPkRba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorization\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype = np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype = np.bool)\n",
        " \n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    x[i, t, char_indices[char]] = 1\n",
        "  y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilbDAUlglC6W",
        "colab_type": "code",
        "outputId": "1be11582-04f9-4676-c2f9-7356bd8bdbf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "x[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False, False, False, ..., False, False,  True],\n",
              "       [ True, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       ...,\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpwTLzyNlRGR",
        "colab_type": "code",
        "outputId": "0a042d2a-fce1-46bd-cfc2-17f10cf3c911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "y[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False,  True, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G088PPqh6oAm",
        "colab_type": "text"
      },
      "source": [
        "# **Building an LSTM Model**\n",
        "Since we will predict 38 unique characters, we need to use softmax activation function on last layer of the model and categorical_crossentropy loss function during training process.\n",
        "\n",
        "return_sequences = True means that we pass the output of one layer to another. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhfbR2dUlQWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape = (maxlen, len(chars)), return_sequences = True))\n",
        "model.add(LSTM(128, return_sequences = True))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(len(chars), activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTEY68dFmVbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(lr = 0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyjzdg3QWRhX",
        "colab_type": "code",
        "outputId": "a31b1f5c-cd9a-4943-eb24-633c1175c93d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size = 512,\n",
        "          epochs = 30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "143235/143235 [==============================] - 155s 1ms/step - loss: 2.6583\n",
            "Epoch 2/30\n",
            "143235/143235 [==============================] - 152s 1ms/step - loss: 1.8726\n",
            "Epoch 3/30\n",
            "143235/143235 [==============================] - 152s 1ms/step - loss: 1.5974\n",
            "Epoch 4/30\n",
            "143235/143235 [==============================] - 153s 1ms/step - loss: 1.4680\n",
            "Epoch 5/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.3906\n",
            "Epoch 6/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.3355\n",
            "Epoch 7/30\n",
            "143235/143235 [==============================] - 153s 1ms/step - loss: 1.2927\n",
            "Epoch 8/30\n",
            "143235/143235 [==============================] - 151s 1ms/step - loss: 1.2553\n",
            "Epoch 9/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.2269\n",
            "Epoch 10/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.2019\n",
            "Epoch 11/30\n",
            "143235/143235 [==============================] - 151s 1ms/step - loss: 1.1812\n",
            "Epoch 12/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.1609\n",
            "Epoch 13/30\n",
            "143235/143235 [==============================] - 147s 1ms/step - loss: 1.1471\n",
            "Epoch 14/30\n",
            "143235/143235 [==============================] - 151s 1ms/step - loss: 1.1323\n",
            "Epoch 15/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.1205\n",
            "Epoch 16/30\n",
            "143235/143235 [==============================] - 149s 1ms/step - loss: 1.1116\n",
            "Epoch 17/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.1004\n",
            "Epoch 18/30\n",
            "143235/143235 [==============================] - 149s 1ms/step - loss: 1.0922\n",
            "Epoch 19/30\n",
            "143235/143235 [==============================] - 151s 1ms/step - loss: 1.0855\n",
            "Epoch 20/30\n",
            "143235/143235 [==============================] - 151s 1ms/step - loss: 1.0814\n",
            "Epoch 21/30\n",
            "143235/143235 [==============================] - 151s 1ms/step - loss: 1.0760\n",
            "Epoch 22/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.0733\n",
            "Epoch 23/30\n",
            "143235/143235 [==============================] - 149s 1ms/step - loss: 1.0681\n",
            "Epoch 24/30\n",
            "143235/143235 [==============================] - 149s 1ms/step - loss: 1.0740\n",
            "Epoch 25/30\n",
            "143235/143235 [==============================] - 149s 1ms/step - loss: 1.0715\n",
            "Epoch 26/30\n",
            "143235/143235 [==============================] - 149s 1ms/step - loss: 1.0714\n",
            "Epoch 27/30\n",
            "143235/143235 [==============================] - 151s 1ms/step - loss: 1.0700\n",
            "Epoch 28/30\n",
            "143235/143235 [==============================] - 151s 1ms/step - loss: 1.0728\n",
            "Epoch 29/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.0771\n",
            "Epoch 30/30\n",
            "143235/143235 [==============================] - 150s 1ms/step - loss: 1.0762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEl37uVwiNY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('first_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6-wiZ3x_jPp",
        "colab_type": "text"
      },
      "source": [
        "# **Training the Language Model and Sampling from it**\n",
        "To generate text from trained language model, we must doing the following repeatedly:\n",
        "\n",
        "\n",
        "1.   Draw from the model a probability distribution for the next character given the generated text available so far.\n",
        "2.   Reweight the distribution to a certain temperature.\n",
        "3.   Sample the next character at random according to the reweighted distribution.\n",
        "4.   Add the new character at the end of the available text.\n",
        "\n",
        "In sampling, temperature value decides entropy of next character prediction. If temperature value is low then entropy and randomness of the next character prediction will be predictable and repetitive. This is called greedy sampling. If the temperature value is high the entropy and randomness of the next character will have high randomness and hard to predict. This is called stochastic sampling.\n",
        "\n",
        "To create a new and different sequences from text, we can use high temperature, on the other hand, to create more for example english-like sequences, we can use low temperature on prediction process. \n",
        "\n",
        "Let's use different scale of temperature to investigate the effect on our character predictions.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV9g9fF8m-VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature = 1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKV__sapnefe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, sentence):\n",
        "  # start_index = rd.randint(0, len(text) - maxlen - 1)\n",
        "  # generated_text = text[start_index: start_index + maxlen]\n",
        "  generated_text = sentence[0: maxlen]\n",
        "  print('\\n\\t--- Generating with seed --> \"' + generated_text + '\"')\n",
        " \n",
        "  for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
        "    print('\\n\\t--- Temperature -->', temperature)\n",
        "    sys.stdout.write(generated_text)\n",
        " \n",
        "    for i in range(200):\n",
        "      sampled = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(generated_text):\n",
        "        sampled[0, t, char_indices[char]] = 1.\n",
        "          \n",
        "      preds = model.predict(sampled)[0]\n",
        "      next_index = sample(preds, temperature)\n",
        "      next_char = chars[next_index]\n",
        " \n",
        "      generated_text += next_char\n",
        "      generated_text = generated_text[1:]\n",
        " \n",
        "      sys.stdout.write(next_char)\n",
        "      sys.stdout.flush()\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNET_4FQDX2K",
        "colab_type": "text"
      },
      "source": [
        "# **Generating Text**\n",
        "When we create a text using and predict from the language model, we can clearly see that, with 0.2 and 0.5 values of temperature, we can generate meaningful words of english and with 1.0 and 1.2 values of temperature, we can generate english-like words.\n",
        "\n",
        "We should not expect to predict meaningful context at this stage because the data and the training epochs are so small. If we train the model with more data and epoch, prediction may have meaningful context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RI3GegepamM",
        "colab_type": "code",
        "outputId": "09201831-edda-49f8-f5df-9635bd6cad50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "generate_text(model, 'beware for i am fearless and therefore powerful i will watch')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--- Generating with seed --> \"beware for i am fearless and therefore powerful i will watch\"\n",
            "\n",
            "\t--- Temperature --> 0.2\n",
            "beware for i am fearless and therefore powerful i will watching a torment or the sensation of the hope of the despair the same an account of the most creator was a wretch i have not wished to the sun feeded she was to assured the sea by the secret he said the \n",
            "\n",
            "\t--- Temperature --> 0.5\n",
            "feeded she was to assured the sea by the secret he said the secret her also be among the secret of communious of marn and the family and the arabiant of\n",
            "your enemy and have surprised i shall be even put the useries of my dear very despair  the envoxute sympath\n",
            "\n",
            "\t--- Temperature --> 1.0\n",
            "ut the useries of my dear very despair  the envoxute sympathy\n",
            "and streaming occur overcammed mr kindless nature when perhithed you\n",
            "to his misery into me seemed to the most of splank\n",
            "younger complying to explain  royage or restraited me all have you will commen\n",
            "\n",
            "\t--- Temperature --> 1.2\n",
            "to explain  royage or restraited me all have you will commenc at an enothor wive sadered puplically when he\n",
            "had had helided to progress in reguarbing paraciect was\n",
            "necessity\n",
            "us you are generatelage and would be peace spectles puugimed\n",
            "again a wippling in my wi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW_hjrU1PrKe",
        "colab_type": "text"
      },
      "source": [
        "# **Inspecting Different Models to improve Text Generation**\n",
        "Let's inspect what we can do to generate more meaningful text generation. To do this, first of all, i want to use Bidirectional layer with LSTM to create a new model.\n",
        "\n",
        "A bidirectional RNN exploits the order sensitivity of  RNNs: it consists of using two regular RNNs, such as the GRU and LSTM layers each of which processes the input sequence in one direction and  then  merging  their  representations. By processing a sequence both ways, a bidirectional RNN can catch patterns thatmay be overlooked by a unidirectional RNN.\n",
        "\n",
        "For Example:\n",
        "Let's say we try to predict the next word in a sentence, on a high level what a unidirectional LSTM will see is\n",
        "* The boys went to ....\n",
        "\n",
        "And will try to predict the next word only by this context, with bidirectional LSTM you will be able to see information further down the road for example\n",
        "\n",
        "Forward LSTM:\n",
        "* The boys went to ...\n",
        "\n",
        "Backward LSTM:\n",
        "* ... and then they got out of the pool\n",
        "\n",
        "You can see that using the information from the future it could be easier for the network to understand what the next word is.\n",
        "\n",
        "In theory, using the Bidirectional layer I can expect to predict better text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZx8vQdvO8gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        " \n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(256, input_shape = (maxlen, len(chars)), return_sequences = True)))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences = True)))\n",
        "model.add(Bidirectional(LSTM(512)))\n",
        "model.add(Dense(len(chars), activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6terEvqlntu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(lr = 0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-7piNQ-lznT",
        "colab_type": "code",
        "outputId": "15e26639-5a3e-42b5-8549-fa30e7acc164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size = 512,\n",
        "          epochs = 30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "143235/143235 [==============================] - 400s 3ms/step - loss: 3.4925\n",
            "Epoch 2/30\n",
            "143235/143235 [==============================] - 398s 3ms/step - loss: 2.9547\n",
            "Epoch 3/30\n",
            "143235/143235 [==============================] - 399s 3ms/step - loss: 2.4176\n",
            "Epoch 4/30\n",
            "143235/143235 [==============================] - 396s 3ms/step - loss: 2.2150\n",
            "Epoch 5/30\n",
            "143235/143235 [==============================] - 399s 3ms/step - loss: 2.0390\n",
            "Epoch 6/30\n",
            "143235/143235 [==============================] - 397s 3ms/step - loss: 1.8273\n",
            "Epoch 7/30\n",
            "143235/143235 [==============================] - 396s 3ms/step - loss: 1.6816\n",
            "Epoch 8/30\n",
            "143235/143235 [==============================] - 397s 3ms/step - loss: 1.5777\n",
            "Epoch 9/30\n",
            "143235/143235 [==============================] - 396s 3ms/step - loss: 1.4977\n",
            "Epoch 10/30\n",
            "143235/143235 [==============================] - 394s 3ms/step - loss: 1.4390\n",
            "Epoch 11/30\n",
            "143235/143235 [==============================] - 396s 3ms/step - loss: 1.3935\n",
            "Epoch 12/30\n",
            "143235/143235 [==============================] - 395s 3ms/step - loss: 1.3583\n",
            "Epoch 13/30\n",
            "143235/143235 [==============================] - 394s 3ms/step - loss: 1.3296\n",
            "Epoch 14/30\n",
            "143235/143235 [==============================] - 395s 3ms/step - loss: 1.3034\n",
            "Epoch 15/30\n",
            "143235/143235 [==============================] - 397s 3ms/step - loss: 1.2864\n",
            "Epoch 16/30\n",
            "143235/143235 [==============================] - 397s 3ms/step - loss: 1.2679\n",
            "Epoch 17/30\n",
            "143235/143235 [==============================] - 394s 3ms/step - loss: 1.2482\n",
            "Epoch 18/30\n",
            "143235/143235 [==============================] - 395s 3ms/step - loss: 1.2317\n",
            "Epoch 19/30\n",
            "143235/143235 [==============================] - 396s 3ms/step - loss: 1.2044\n",
            "Epoch 20/30\n",
            "143235/143235 [==============================] - 394s 3ms/step - loss: 1.2058\n",
            "Epoch 21/30\n",
            "143235/143235 [==============================] - 393s 3ms/step - loss: 1.1963\n",
            "Epoch 22/30\n",
            "143235/143235 [==============================] - 393s 3ms/step - loss: 1.1900\n",
            "Epoch 23/30\n",
            "143235/143235 [==============================] - 393s 3ms/step - loss: 1.1811\n",
            "Epoch 24/30\n",
            "143235/143235 [==============================] - 397s 3ms/step - loss: 1.1739\n",
            "Epoch 25/30\n",
            "143235/143235 [==============================] - 393s 3ms/step - loss: 1.2195\n",
            "Epoch 26/30\n",
            "143235/143235 [==============================] - 394s 3ms/step - loss: 1.1482\n",
            "Epoch 27/30\n",
            "143235/143235 [==============================] - 399s 3ms/step - loss: 1.1458\n",
            "Epoch 28/30\n",
            "143235/143235 [==============================] - 390s 3ms/step - loss: 1.1369\n",
            "Epoch 29/30\n",
            "143235/143235 [==============================] - 392s 3ms/step - loss: 1.1325\n",
            "Epoch 30/30\n",
            "143235/143235 [==============================] - 396s 3ms/step - loss: 1.1190\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f31c5fd2048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlbDu2QWl6-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('second_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COBnlHmlmmDI",
        "colab_type": "code",
        "outputId": "b5928050-c9c9-48fe-c287-5c4a00cc6beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "generate_text(model, 'beware for i am fearless and therefore powerful i will watch')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--- Generating with seed --> \"beware for i am fearless and therefore powerful i will watch\"\n",
            "\n",
            "\t--- Temperature --> 0.2\n",
            "beware for i am fearless and therefore powerful i will watchfing the straw and the monster and i in the beings of the greatest warmine and the secoment of the greatest\n",
            "aspect the fiend of the greatest\n",
            "and the sea and i am my feelings and intervered me and i wa\n",
            "\n",
            "\t--- Temperature --> 0.5\n",
            "\n",
            "and the sea and i am my feelings and intervered me and i was the lovely plan’\n",
            "\n",
            "i was sooth you even it is\n",
            "the truth of the\n",
            "most destrain had been\n",
            "to be a few heaven in the peet i am no among the coppanion  i had been visited in the house who was more and one \n",
            "\n",
            "\t--- Temperature --> 1.0\n",
            "anion  i had been visited in the house who was more and one of sight\n",
            "\n",
            "and not man arguilling i did not be it gazed on the summit of domes on your gentle\n",
            "also and supposite and in the more ceas me and"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7OO_XarSdMm",
        "colab_type": "text"
      },
      "source": [
        "# **Generating Text**\n",
        "We can clearly say that the new text generation is worse than before for all temperature value. I think the reason behind this is that we stuck in local minimum when training the model. Loss value is not decreasing much after about 20th epoch. Let's try to solve this problem and generate better text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNjNvJMTrph",
        "colab_type": "text"
      },
      "source": [
        "# **Inspecting Different Parameters to improve Text Generation**\n",
        "I change loss value to 'adam' for decreasing loss value as much as possible in training process.\n",
        "\n",
        "Let's examine whether we have made progress or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9lw6BM1mqk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        " \n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(64, input_shape = (maxlen, len(chars)), return_sequences = True)))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
        "model.add(Bidirectional(LSTM(256)))\n",
        "model.add(Dense(len(chars), activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy_CbMRM7RAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcm9NuF_7yfu",
        "colab_type": "code",
        "outputId": "811f2357-d02e-4e8b-e2e5-dc95578b1d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x, y, batch_size = 512, epochs = 30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "143235/143235 [==============================] - 164s 1ms/step - loss: 2.6974\n",
            "Epoch 2/30\n",
            "143235/143235 [==============================] - 164s 1ms/step - loss: 2.1996\n",
            "Epoch 3/30\n",
            "143235/143235 [==============================] - 160s 1ms/step - loss: 1.9728\n",
            "Epoch 4/30\n",
            "143235/143235 [==============================] - 159s 1ms/step - loss: 1.8223\n",
            "Epoch 5/30\n",
            "143235/143235 [==============================] - 160s 1ms/step - loss: 1.7265\n",
            "Epoch 6/30\n",
            "143235/143235 [==============================] - 161s 1ms/step - loss: 1.6494\n",
            "Epoch 7/30\n",
            "143235/143235 [==============================] - 164s 1ms/step - loss: 1.5863\n",
            "Epoch 8/30\n",
            "143235/143235 [==============================] - 159s 1ms/step - loss: 1.5266\n",
            "Epoch 9/30\n",
            "143235/143235 [==============================] - 158s 1ms/step - loss: 1.4765\n",
            "Epoch 10/30\n",
            "143235/143235 [==============================] - 161s 1ms/step - loss: 1.4319\n",
            "Epoch 11/30\n",
            "143235/143235 [==============================] - 162s 1ms/step - loss: 1.3967\n",
            "Epoch 12/30\n",
            "143235/143235 [==============================] - 161s 1ms/step - loss: 1.3588\n",
            "Epoch 13/30\n",
            "143235/143235 [==============================] - 161s 1ms/step - loss: 1.3266\n",
            "Epoch 14/30\n",
            "143235/143235 [==============================] - 163s 1ms/step - loss: 1.3013\n",
            "Epoch 15/30\n",
            "143235/143235 [==============================] - 162s 1ms/step - loss: 1.2689\n",
            "Epoch 16/30\n",
            "143235/143235 [==============================] - 163s 1ms/step - loss: 1.2380\n",
            "Epoch 17/30\n",
            "143235/143235 [==============================] - 164s 1ms/step - loss: 1.2136\n",
            "Epoch 18/30\n",
            "143235/143235 [==============================] - 167s 1ms/step - loss: 1.1862\n",
            "Epoch 19/30\n",
            "143235/143235 [==============================] - 164s 1ms/step - loss: 1.1624\n",
            "Epoch 20/30\n",
            "143235/143235 [==============================] - 164s 1ms/step - loss: 1.1366\n",
            "Epoch 21/30\n",
            "143235/143235 [==============================] - 165s 1ms/step - loss: 1.1093\n",
            "Epoch 22/30\n",
            "143235/143235 [==============================] - 164s 1ms/step - loss: 1.0865\n",
            "Epoch 23/30\n",
            "143235/143235 [==============================] - 170s 1ms/step - loss: 1.0587\n",
            "Epoch 24/30\n",
            "143235/143235 [==============================] - 169s 1ms/step - loss: 1.0365\n",
            "Epoch 25/30\n",
            "143235/143235 [==============================] - 167s 1ms/step - loss: 1.0103\n",
            "Epoch 26/30\n",
            "143235/143235 [==============================] - 165s 1ms/step - loss: 0.9892\n",
            "Epoch 27/30\n",
            "143235/143235 [==============================] - 164s 1ms/step - loss: 0.9603\n",
            "Epoch 28/30\n",
            "143235/143235 [==============================] - 168s 1ms/step - loss: 0.9357\n",
            "Epoch 29/30\n",
            "143235/143235 [==============================] - 165s 1ms/step - loss: 0.9155\n",
            "Epoch 30/30\n",
            "143235/143235 [==============================] - 164s 1ms/step - loss: 0.8854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fcc0f2986d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIVv9HnJ-LT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('third_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghsbFxTmO53V",
        "colab_type": "code",
        "outputId": "d85fe983-28be-43b1-f431-e8f0f5e9c63c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        " generate_text(model, 'beware for i am fearless and therefore powerful i will watch')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--- Generating with seed --> \"beware for i am fearless and therefore powerful i will watch\"\n",
            "\n",
            "\t--- Temperature --> 0.2\n",
            "beware for i am fearless and therefore powerful i will watching and one of the most friends which i had formed the most felix was paid the father the sun doel content to project gutenbergtm electronic works in the maintenance of the most friends when i reflect\n",
            "\n",
            "\t--- Temperature --> 0.5\n",
            " works in the maintenance of the most friends when i reflected the histreas of a sometimes i said the subst that i am hardly knees her miserable and a cursed by regater so on the project gutenbergtm\n",
            "curiosity and far more my eyes that feelings as that she\n",
            "foll\n",
            "\n",
            "\t--- Temperature --> 1.0\n",
            "uriosity and far more my eyes that feelings as that she\n",
            "followed yet one of the facts of donations that was on the horsen of the room weigh on the most carorsed by a thousand fear upon\n",
            "joy state or the slass\n",
            "hirstorined for there and that of her\n",
            "\n",
            "chelgest caro\n",
            "\n",
            "\t--- Temperature --> 1.2\n",
            "e slass\n",
            "hirstorined for there and that of her\n",
            "\n",
            "chelgest carolely desire to breasings at the\n",
            "companions of which i fearlents\n",
            "strength and another prised me’less  the moon sailogy and distinguish that was\n",
            "deluced reliling  the evening with those no maffiring\n",
            "sel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCa4yZDuUsxD",
        "colab_type": "text"
      },
      "source": [
        "# **Generating Text**\n",
        "We can clearly see that we made progress on loss value and character prediction according to preceding model. Again, for 0.2 and 0.5 of temperature values, the model generated english words. For 1.0 and 1.2 of temperature values, the model generated english-like, random words.\n",
        "\n",
        "# **Conclusion and Future Works**\n",
        "We can accept first and third models to generate a \"Frankenstein-like\" text as beginning step. \n",
        "\n",
        "To improve writing skills of the model, we must train the model with more text data, more layers and more epoch like over 100. "
      ]
    }
  ]
}